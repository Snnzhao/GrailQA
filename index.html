<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>Strongly Generalizable Question Answering Dataset</title><meta name="description" content="Strongly Generalizable Question Answering Dataset (GrailQA) is a new large-scale, high-quality KBQA dataset with 64,495 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.png"><link rel="image_src" type="image/png" href="/GrailQA/logo.png"><link rel="shortcut icon" href="/GrailQA/favicon.ico" type="image/x-icon"><link rel="icon" href="/GrailQA/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/GrailQA/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/GrailQA/stylesheets/layout.css"><link rel="stylesheet" href="/GrailQA/stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/GrailQA/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/GrailQA/">Home</a></li><li><a href="/GrailQA/explore/grailqa">Explore GrailQA</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/GrailQA/">GrailQA</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">SQuAD<b>2.0</b></h1><h2 id="appSubtitle">The Stanford Question Answering Dataset</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is GrailQA?</h2></div><p> <span><Strongly></Strongly><b>G</b>eneralizable <b>Q</b>uestion <b>A</b>nswering <Dataset>(GrailQA)</Dataset></span>is a new large-scale, high-quality KBQA dataset with 64,495 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.</p><hr><p></p><a class="btn actionBtn" href="/GrailQA/explore/grailqa">Explore GrailQA</a><a class="btn actionBtn" href="http://arxiv.org/abs/1806.03822">GrailQA paper (Gu et al. '20)</a><hr><div class="infoHeadline"><h2>Getting Started</h2></div><p>We've built a few resources to help you get started with the dataset.</p><p> Download a copy of the dataset (distributed under the  <a href="http://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0</a> license):<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://www.dropbox.com/s/d142oxkv5g9mwxu/grailqa_train.json?dl=0" download>Training Set (179 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.dropbox.com/s/m6l4idm55l2tq0g/grailqa_dev.json?dl=0" download>Dev Set (24 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="https://www.dropbox.com/s/r3qrh1wr611yqbq/grailqa_test_public.json?dl=0" download>Test Set without Labels (5 MB)</a></li></ul></p><p>To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use <code>python evaluate-v2.0.py &lt;path_to_dev-v2.0&gt; &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" download>Evaluation Script v2.0</a></li><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/" download>Sample Prediction File (on Dev v2.0)</a></li></ul></p><p>Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the labels of test set to the public. Here's a tutorial walking you through official evaluation of your model:</p><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0xd51b9aa5cf374ee598f1d6422cd976f3">Submission Tutorial</a><p>Because GrailQA is an ongoing effort, we expect the dataset to evolve.</p><p>To keep up to date with major changes to the dataset, please subscribe:</p><div id="mc_embed_signup"><form class="validate" id="mc-embedded-subscribe-form" action="//google.us13.list-manage.com/subscribe/post?u=1842e6560d6e10316b4e1aaf5&amp;id=76586bdcf4" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate=""><div id="mc_embed_signup_scroll"><input class="email" id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="email address" required=""><div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1842e6560d6e10316b4e1aaf5_76586bdcf4" tabindex="-1" value=""></div><div class="clear"><input class="button" id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe"></div></div></form></div><div class="infoHeadline"><h2>Have Questions?</h2></div><p> Send an email to<a href="mailto:gu.826@osu.edu">gu.826@osu.edu.</a></p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanford-qa.com" data-text="The Stanford Question Answering Dataset - 100,000+ questions for reading comprehension" data-via="stanfordnlp" data-size="large" data-hashtags="SQuAD">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/rajpurkar/SQuAD-explorer" data-icon="octicon-star" data-style="mega" data-count-href="/rajpurkar/SQuAD-explorer/stargazers" data-count-api="/repos/rajpurkar/SQuAD-explorer#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star rajpurkar/SQuAD-explorer on GitHub">Star</a></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><p>SQuAD2.0 tests the ability of a system to not only answer reading comprehension questions, but also abstain when presented with a question that cannot be answered based on the provided paragraph.</p><table class="table performanceTable"><tr><th>Rank</th><th>Model</th><th>EM</th><th>F1</th></tr></table></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="rightNav"><div><ul class="nav navbar-nav navbar-right"><li><a href="/GrailQA/">SQuAD</a></li><li><a href="http://nlp.stanford.edu">Stanford NLP Group</a></li></ul></div></div></div></nav><script src="/GrailQA/bower_components/jquery/dist/jquery.min.js"></script><script src="/GrailQA/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>